{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ujson as json\n",
    "import gzip, itertools, numbers\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import base, utils\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor, ElasticNet, Ridge, LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Predicting Star Ratings\n",
    "\n",
    "Our objective is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a dataset of venue popularities provided by Yelp.  The dataset contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. Note that the venues are not limited to restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO install aws-packages and find an alternative data-source\n",
    "#!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a gzipped file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "    \n",
    "star_ratings = [row['stars'] for row in data]\n",
    "\n",
    "X, Y = utils.shuffle(data, star_ratings, random_state=41)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42, stratify = star_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. city_avg\n",
    "The venues belong to different cities. You can image that the ratings in some cities are probably higher than others. We wish to build an estimator to make a prediction based on this, but first we need to work out the average rating for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityEstimator()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CityEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = defaultdict(int)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        star_sum = defaultdict(int)\n",
    "        count = defaultdict(int)\n",
    "\n",
    "        for row, stars in zip(X, y):\n",
    "            star_sum[row['city']] += stars\n",
    "            count[row['city']] += 1    \n",
    "        for k in star_sum:\n",
    "            self.avg_stars[k] = star_sum[k]/float(count[k]) \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.avg_stars[row['city']] for row in X] # X has to be a pd.DataFrame or a Dictionary !!!!\n",
    "        #return [ self.avg_stars[row['city']] if row['city'] in self.avg_stars else np.nan for row in X ]\n",
    "\n",
    "city_avg = CityEstimator()\n",
    "city_avg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. lat_long_model\n",
    "You can imagine that a city-based model might not be sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  Use the latitude and longitude of a venue as features that help you understand neighborhood dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    '''\n",
    "    extract selected key-value pairs from a dictionary and transform them to an array\n",
    "    '''\n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): # every row of X is a dictionary\n",
    "        data = [ [row[col_name] for col_name in self.col_names] for row in X ]\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use k_nn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_knn__n_neighbors': 54}\n",
      "-0.777439799843\n",
      "r2_score_train = 0.05 r2_score_test = 0.03\n"
     ]
    }
   ],
   "source": [
    "knn_pipe = Pipeline([\n",
    "        ('my_cst',ColumnSelectTransformer(['latitude', 'longitude'])), \n",
    "        ('my_knn',KNeighborsRegressor(n_neighbors=5)) \n",
    "        ])\n",
    "\n",
    "expres_cv = model_selection.ShuffleSplit(n_splits=3, test_size=0.2, random_state=41)\n",
    "\n",
    "parameters = {'my_knn__n_neighbors': range(34,55,4)}\n",
    "\n",
    "gs_latlong_knn = GridSearchCV(knn_pipe, param_grid=parameters, cv=expres_cv, scoring='neg_mean_squared_error', n_jobs=1) \n",
    "\n",
    "gs_latlong_knn.fit(X_train, Y_train)\n",
    "\n",
    "print(gs_latlong_knn.best_params_)\n",
    "print(gs_latlong_knn.best_score_)\n",
    "\n",
    "Y_train_predict = gs_latlong_knn.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_latlong_knn.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %( float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. category_model\n",
    "While location is important, we could also try seeing how predictive the venue's category is. Build an estimator that considers only the categories.\n",
    "\n",
    "The categories come as a list of strings, but the built-in estimators all need numeric input.  The standard way to deal with categorical features is **one-hot encoding**, also known as dummy variables.  In this approach, each category gets its own column in the feature matrix. If the row has a given category, that column gets filled with a 1.  Otherwise, it is 0.\n",
    "\n",
    "The `ColumnSelectTransformer` from the previous question can be used to extract the categories column as a list of strings.  Scikit Learn provides [DictVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which takes in a list of dictionaries.  It creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it.  Missing keys are filled with zeros.  Therefore, we need only build a transformer that takes a list strings to a dictionary with keys given by those strings and values one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_Ridge__alpha': 10.0}\n",
      "0.171877621576\n",
      "alpha = 1000.0000 r2_score_train = 0.20 r2_score_test = 0.17\n"
     ]
    }
   ],
   "source": [
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):  # this will work only if I pass to it the 'category' column/key\n",
    "        return [ { el:1 for el in col} for row in X for col in row ]\n",
    "    \n",
    "\n",
    "category_pipe = Pipeline([\n",
    "        ('my_col_select', ColumnSelectTransformer(['categories'])), \n",
    "        ('my_Dict_enc', DictEncoder()),\n",
    "        ('my_Dict_vect', DictVectorizer(sparse=False) ),\n",
    "        ('my_Ridge', Ridge(alpha=0.1)) \n",
    "        ])\n",
    "\n",
    "\n",
    "parameters = {'my_Ridge__alpha': np.logspace(-1, 2, 7) }\n",
    "gs_category = GridSearchCV(category_pipe, parameters, cv=3, n_jobs=1) #scoring='neg_mean_squared_error', \n",
    "gs_category.fit(X_train, Y_train)        \n",
    "        \n",
    "print(gs_category.best_params_)\n",
    "print(gs_category.best_score_)\n",
    "\n",
    "gs_category.best_estimator_\n",
    "\n",
    "Y_train_predict = gs_category.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_category.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. attribute_model\n",
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them with one-hot encoding.  The `DictVectorizer` can do this, but only once we've flattened the dictionary to a single level, like so:\n",
    "```\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "\n",
    "Build a custom transformer that flattens the attributes dictionary.  Place this in a pipeline with a `DictVectorizer` and a regressor.\n",
    "\n",
    "You may find it difficult to find a single regressor that does well enough.  A common solution is to use a linear model to fit the linear part of some data, and use a non-linear model to fit the residual that the linear model can't fit.  Build a residual estimator that takes as an argument two other estimators.  It should use the first to fit the raw data and the second to fit the residuals of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################### feed with one feature that contains\n",
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):  # this will work only if I pass to it the 'category' column/key\n",
    "        return [ { el:1 for el in col} for row in X for col in row ]\n",
    "\n",
    "#################################################################### feed with ColumnSelectTransformer(['attributes'])\n",
    "class Attrib_Flatten(base.BaseEstimator, base.TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self    \n",
    "    def transform(self, X): \n",
    "        return [ { k1:v1 for k,v in col.iteritems() for k1,v1 in Dict_Maker( k, v ).iteritems() } for row in X for col in row]\n",
    "        \n",
    "def Dict_Maker( k, v ):          # returns {k1:v1, k2:v2, .... , kn:vn}\n",
    "    if isinstance(v, basestring):\n",
    "        return { k + '_' + v : 1}\n",
    "    if isinstance(v, bool):\n",
    "        return { k : int(v==True)} \n",
    "    if isinstance(v, numbers.Number):\n",
    "        return { k : v}\n",
    "    if isinstance(v, dict):\n",
    "        return { k2:v2      for k1,v1 in v.iteritems() for k2,v2 in Dict_Maker( k + '_' + k1, v1 ).iteritems() }\n",
    "    if isinstance(v, list):\n",
    "        return { k2:v2      for v1 in v for k2,v2 in Dict_Maker(k,v1).iteritems() } # read additional footnotes [1]\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# [1] in this case we assume that  all elements in v are strings or dictionaries\n",
    "# in the other cases the algotithm wont brake but it will produce less Key-Value pairs\n",
    "# Example:                k:[v1, v2, v3] -> k:v3 if v1, v2, v3 are integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Use Logistic Regression\n",
    "The ratings can take only the values [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.0001 r2_score_train = -0.06 r2_score_test = -0.06\n",
      "C = 0.0010 r2_score_train = -0.59 r2_score_test = -0.61\n",
      "C = 0.0100 r2_score_train = -0.59 r2_score_test = -0.60\n",
      "C = 0.1000 r2_score_train = -0.59 r2_score_test = -0.62\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "    \n",
    "class another_Y_trf(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.my_map = {} # we could also use defaultdict ...\n",
    "        self.my_inv_map = {}\n",
    "    \n",
    "    def fit(self, Y):\n",
    "        for idx, el in enumerate( list(set(Y)) ):\n",
    "            self.my_map[el]        = idx+1\n",
    "            self.my_inv_map[idx+1] = el\n",
    "        return self\n",
    "    \n",
    "    def transform(self, Y):\n",
    "        return np.array([ self.my_map[el] for el in Y ])\n",
    "    \n",
    "    def inv_trf(self, Y_trf):\n",
    "        return np.array([ self.my_inv_map[idx] for idx in Y_trf ])\n",
    "    \n",
    "    \n",
    "    \n",
    "pipe_attributes_LogR = Pipeline([\n",
    "    ('my_cst', ColumnSelectTransformer(['attributes']) ), \n",
    "    ('my_At_Flat', Attrib_Flatten() ),\n",
    "    ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "    ('my_Logistic', LogisticRegression(C=0.1, n_jobs=-1, max_iter=100, solver='sag'))\n",
    "    ])\n",
    "\n",
    "my_Y_trf = another_Y_trf()\n",
    "my_Y_trf.fit(Y_train)\n",
    "\n",
    "YY_train = my_Y_trf.transform(Y_train)\n",
    "YY_test  = my_Y_trf.transform(Y_test )\n",
    "\n",
    "for C in np.logspace(-4,-1,4):\n",
    "    pipe_attributes_LogR.set_params( my_Logistic__C=C )\n",
    "    \n",
    "    pipe_attributes_LogR.fit(X_train, YY_train)\n",
    "    \n",
    "    Y_train_predict = my_Y_trf.inv_trf( pipe_attributes_LogR.predict(X_train) )\n",
    "    Y_test_predict  = my_Y_trf.inv_trf( pipe_attributes_LogR.predict(X_test ) )\n",
    "\n",
    "    print 'C = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(C, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Use Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_Ridge__alpha': 10.0}\n",
      "0.0704371250279\n",
      "alpha = 100.0000 r2_score_train = 0.07 r2_score_test = 0.07\n"
     ]
    }
   ],
   "source": [
    "pipe_attributes_LinR = Pipeline([\n",
    "    ('my_cst', ColumnSelectTransformer(['attributes']) ), \n",
    "    ('my_At_Flat', Attrib_Flatten() ),\n",
    "    ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "    #('my_PCA', PCA(n_components=n)),\n",
    "    ('my_Ridge', Ridge(alpha=1.0))\n",
    "    ])\n",
    "\n",
    "parameters = {'my_Ridge__alpha': np.logspace(-3, 2, 6) }\n",
    "gs_attribute_LinR = GridSearchCV(pipe_attributes_LinR, parameters, cv=3, n_jobs=1)\n",
    "gs_attribute_LinR.fit(X_train, Y_train)        \n",
    "        \n",
    "print(gs_attribute_LinR.best_params_)\n",
    "print(gs_attribute_LinR.best_score_)\n",
    "\n",
    "\n",
    "Y_train_predict = gs_attribute_LinR.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_attribute_LinR.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Use RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_RF__n_estimators': 100, 'my_RF__max_depth': 10}\n",
      "0.0831967040806\n",
      "alpha = 100.0000 r2_score_train = 0.12 r2_score_test = 0.09\n"
     ]
    }
   ],
   "source": [
    "pipe_attributes_RF = Pipeline([ \n",
    "    ('my_cst', ColumnSelectTransformer(['attributes']) ), \n",
    "    ('my_At_Flat', Attrib_Flatten() ),\n",
    "    ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "    ('my_RF', RandomForestRegressor(n_jobs = 1, max_depth=10))\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {'my_RF__n_estimators': [20, 50, 100], \n",
    "              'my_RF__max_depth': [2, 6, 10]\n",
    "             }\n",
    "\n",
    "\n",
    "gs_attribute_RF = GridSearchCV(pipe_attributes_RF, parameters, cv=3, n_jobs=1)\n",
    "gs_attribute_RF.fit(X_train, Y_train)        \n",
    "        \n",
    "print(gs_attribute_RF.best_params_)\n",
    "print(gs_attribute_RF.best_score_)\n",
    "\n",
    "\n",
    "Y_train_predict = gs_attribute_RF.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_attribute_RF.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Use k_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my_knn__n_neighbors': 50}\n",
      "0.0654473572302\n",
      "alpha = 100.0000 r2_score_train = 0.08 r2_score_test = 0.07\n"
     ]
    }
   ],
   "source": [
    "pipe_attributes_knn = Pipeline([ \n",
    "    ('my_cst', ColumnSelectTransformer(['attributes']) ), \n",
    "    ('my_At_Flat', Attrib_Flatten() ),\n",
    "    ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "    ('my_svd', TruncatedSVD(n_components=30) ),\n",
    "    ('my_knn', KNeighborsRegressor(n_neighbors=5) )\n",
    "    ])\n",
    "\n",
    "\n",
    "parameters = {'my_knn__n_neighbors': [20, 30, 40, 50]}\n",
    "\n",
    "gs_attribute_knn = GridSearchCV( pipe_attributes_knn, param_grid = parameters, cv=3, n_jobs=1)\n",
    "gs_attribute_knn.fit(X_train, Y_train) \n",
    "        \n",
    "print(gs_attribute_knn.best_params_)\n",
    "print(gs_attribute_knn.best_score_)\n",
    "\n",
    "\n",
    "Y_train_predict = gs_attribute_knn.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_attribute_knn.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Create a pipeline containing linear and nonlinear models where the nonlinear model try to decrease the residua from the linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnsembleTransformer(base.BaseEstimator, base.TransformerMixin):    \n",
    "    \n",
    "    def __init__(self, base_estimator, residual_estimator_1, residual_estimator_2):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.residual_estimator_1 = residual_estimator_1\n",
    "        self.residual_estimator_2 = residual_estimator_2\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.base_estimator.fit(X, y)\n",
    "        y_err = y - self.base_estimator.predict(X)\n",
    "        \n",
    "        self.residual_estimator_1.fit(X, y_err)\n",
    "        self.residual_estimator_2.fit(X, y_err)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        all_ests = [ self.base_estimator, self.residual_estimator_1, self.residual_estimator_2 ]\n",
    "        return np.array([est.predict(X) for est in all_ests]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_pipe = Pipeline([\n",
    "        ('ensemble', EnsembleTransformer(base_estimator = gs_attribute_LinR.best_estimator_, \n",
    "                                         residual_estimator_1 = gs_attribute_RF.best_estimator_, \n",
    "                                         residual_estimator_2 = gs_attribute_knn.best_estimator_  ) ),\n",
    "        ('blend', Ridge(alpha=0))\n",
    "        ])\n",
    "\n",
    "parameters = {'my_knn__n_neighbors': range(10,59,4) }\n",
    "\n",
    "\n",
    "gs_attribute_ensemble = GridSearchCV( ensemble_pipe, param_grid = parameters, cv=3, n_jobs=1)\n",
    "gs_attribute_ensemble.fit(X_train, Y_train)    \n",
    "        \n",
    "print(gs_attribute_ensemble.best_params_)\n",
    "print(gs_attribute_ensemble.best_score_ )\n",
    "\n",
    "\n",
    "Y_train_predict = gs_attribute_ensemble.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = gs_attribute_ensemble.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. full_model\n",
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to an estimator, we will have to turn them into transformers.  (A pipeline can contain at most a single estimator.)  Build a custom `ModelTransformer` class that takes an estimator as an argument.  When `fit()` is called, the estimator should be fit.  When `transform()` is called, the estimator's `predict()` method should be called, and its results returned.\n",
    "\n",
    "Note that the output of the `transform()` method should be a 2-D array with a single column, in order for it to work well with the Scikit Learn pipeline.  If you're using Numpy arrays, you can use `.reshape(-1, 1)` to create a column vector.  If you are just using Python lists, you will want a list of lists of single elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EstimatorTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array(   self.estimator.predict(X)  ).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change ColumnSelectTransform to return np.array instead of array\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "pipe_LL = Pipeline([('my_cst',ColumnSelectTransformer(['latitude', 'longitude'])), \n",
    "                    ('my_knn',KNeighborsRegressor(n_neighbors=40)) ])\n",
    "\n",
    "pipe_Cateory = Pipeline([('my_cst', ColumnSelectTransformer(['categories'])), \n",
    "                         ('my_Denc', DictEncoder()),\n",
    "                         ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "                         ('my_Ridge', Ridge(alpha=6)) ])\n",
    "\n",
    "pipe_Attributes = Pipeline([('my_cst', ColumnSelectTransformer(['attributes']) ), \n",
    "                            ('my_At_Flat', Attrib_Flatten() ),\n",
    "                            ('my_DVect', DictVectorizer(sparse=False) ),\n",
    "                            ('my_RF', ensemble.RandomForestRegressor(max_features=20, n_estimators=500, min_samples_leaf=4))])\n",
    "\n",
    "\n",
    "ET_CityAvg    = EstimatorTransformer(city_avg)\n",
    "\n",
    "ET_LatLong    = EstimatorTransformer(gs_latlong_knn.best_estimator_)\n",
    "\n",
    "ET_Cateory    = EstimatorTransformer(gs_category.best_estimator_)\n",
    "\n",
    "ET_Attributes = EstimatorTransformer(gs_attribute_ensemble.best_estimator_)\n",
    "\n",
    "union = FeatureUnion([ ('ET_CityAvg',    ET_CityAvg),  \n",
    "                       ('ET_LatLong',    ET_LatLong), \n",
    "                       ('ET_Cateory',    ET_Cateory), \n",
    "                       ('ET_Attributes', ET_Attributes) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use a pipeline to combine the feature union with a linear regression (or another model) to weight the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_pipe = Pipeline([\n",
    "        ('ensemble', union),\n",
    "        ('blend', Ridge(alpha=0))\n",
    "        ])\n",
    "\n",
    "ensemble_pipe.fit(X_train, Y_train)\n",
    "\n",
    "Y_train_predict = ensemble_pipe.best_estimator_.predict(X_train)\n",
    "Y_test_predict  = ensemble_pipe.best_estimator_.predict(X_test )\n",
    "\n",
    "print 'alpha = %.4f r2_score_train = %.2f r2_score_test = %.2f'\\\n",
    "            %(alpha, float(r2_score(Y_train, Y_train_predict)), float(r2_score(Y_test, Y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
